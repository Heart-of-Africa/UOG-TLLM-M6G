import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# åŠ è½½ç¬¬0å±‚æ¨¡å‹
model_path = "./checkpoints/layer_00"
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)
model.eval().cuda()

# æ¨ç†å‡½æ•°
def generate(text, max_length=50):
    inputs = tokenizer(text, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.7
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ç¤ºä¾‹
if __name__ == "__main__":
    prompt = "è§£æ–¹ç¨‹ x^2 + 2x + 1 = 0 çš„æ ¹æ˜¯"
    print("ğŸ§  æ¨¡å‹å›ç­”ï¼š", generate(prompt))
